---
title: "Project 1"
output: html_notebook
---

```{r}
library(car)# avPlots
library(carData)
library(MASS)
library(ggplot2)
library(plotly)  
library(dplyr)
library(GGally)  # ggpairs
library(leaps)
library(olsrr)
library(caret)
library(glmnet)
```


# All models
Load data

```{r}
bodyF <- read.csv("./bodyfatmen.csv",
               header = TRUE)

View(bodyF)
n <- nrow(bodyF)
p <- ncol(bodyF) # k+1

```



Create full model
```{r}
bodyF.model <- lm(density ~ ., data = bodyF)
summary(bodyF.model)
```
We see that the model have a lot of confidence with abdomen and wrist but no so much with height, chest, hip, knee, ankle and biceps

## Residuals analysis

```{r}
# Normal probability plot of residuals
plot(bodyF.model, which=2)
```
Seems pretty good, so we have a relatively strong reason to beleive the normality assumtion of the residual. However we see a bit of light tails. The points 39, 203 and 220 could be outliers since they do not follow the normality hypothesis (but only 203 and 220 seem more severe).


```{r}
# Residuals vs. fitted values
plot(bodyF.model, which=c(1,3))
plot(studres(bodyF.model), xlab="Fitted values", ylab="Studentized residual")
plot(rstudent(bodyF.model), xlab="Fitted values", ylab="R-Student residual")
```
Semms very good, we do not see any aparent shape, which corresponds to a uncorrelated fitted values with the residuals. We can see in the standarized residuals vs fitted values, that the points 39, 203, and 220 have a standarized residual higher than the variance, which could indicate us that they are outliers.

These plots also indicate us that we might not need to transform our y variable. Nevertheless we can try to find the sugested transformation by the Cox-Box method to further confrirm our hypothesis.

```{r}
bc <- boxCox(bodyF.model, lambda=seq(-10, 3))
bc$x[which(bc$y==max(bc$y))]
```
We observe that the power which maximixes the maximum likelihood is -4. However, we see that the CI are relatively long and that they also contain 1.

We can try using the transformation of y^-4 and see if it improves the prior plots.
```{r}
bodyF.extra <- bodyF
bodyF.extra$density <- (bodyF$density)^(-4)
```

```{r}
bodyF.model.trans <- lm(density ~ ., data = bodyF.extra)
summary(bodyF.model.trans)
```

```{r}
# Normal probability plot of residuals
plot(bodyF.model.trans, which=2)
# Residuals vs. fitted values
plot(bodyF.model.trans, which=c(1,3))
plot(studres(bodyF.model.trans), xlab="Fitted values", ylab="Studentized residual")
plot(rstudent(bodyF.model.trans), xlab="Fitted values", ylab="R-Student residual")
```
As we can see there is not a considerable improvement, neither in fixing the light tails. In addition to the long CI of the Box-Cox, we consider not necessary to do any transformation to the y varaiable.



```{r}
### Horizontal band, so satisfactory distribution
#par(mfrow = c(3, 3))
# Residuals vs. regressor variables
plot(bodyF$age, bodyF.model$residuals)
plot(bodyF$weight, bodyF.model$residuals)
plot(bodyF$height, bodyF.model$residuals)
plot(bodyF$neck, bodyF.model$residuals)
plot(bodyF$chest, bodyF.model$residuals)
plot(bodyF$abdomen, bodyF.model$residuals)
plot(bodyF$hip, bodyF.model$residuals)
plot(bodyF$thigh, bodyF.model$residuals)
plot(bodyF$knee, bodyF.model$residuals)
plot(bodyF$ankle, bodyF.model$residuals)
plot(bodyF$biceps, bodyF.model$residuals)
plot(bodyF$forearm, bodyF.model$residuals)
plot(bodyF$wrist, bodyF.model$residuals)
```
We see that all the plots have a rectangular shape, indicating that the uncorrelation between the regresors and the residuals.

```{r}
# Partial residual plots
avPlots(bodyF.model) # need to press enter in terminal!
```
These plots, with the previous ones, shows us that it might not be necesary to transform our regressors since all the plots follow a line.
Also we see that that the slope of the height, chest and knee is zero, while weight, neck, abdomen, hip, forearm (due to outliers) and wrist have greater slope. Thus, this indicates that we should priorize the variables with higher slope.


```{r}
# PRESS statistics

pr <- resid(bodyF.model)/(1 - lm.influence(bodyF.model)$hat)
PRESS <- sum(pr^2) 
SSt <- sum((bodyF$density - mean(bodyF$density))^2)
R2prediction <- 1 - PRESS/SSt
print(R2prediction)
```
We could expect this model to explain about 70.52% of the variability in predicting new observations. Which is not as desirable, but it is not that much of a trade off compared to the R2 0.7451.



## Leverage and outliers


```{r}
# Define cutoff
leverage.cutoff <- 2 * p / n  # MPV p. 213
cooks.cutoff <- qf(0.5, p, n - p, lower.tail = FALSE)  # MPV p. 215
dfbetas.cutoff <- 2 / sqrt(n)  # MPV p. 218
dffits.cutoff <- 2 * sqrt(p / n)  # MPV p. 219
studres.cutoff <- qt(0.05 / 2, n - p, lower.tail = FALSE)  # MPV p. 135
```

```{r}
### leverage points
bodyF.hat <- hatvalues(bodyF.model)
a <-bodyF.hat[bodyF.hat > leverage.cutoff]
print(a)
View(bodyF[names(a),])
```



```{r}
plot(bodyF.model, which=5)
```
We can see that none of the points are considered as influentials based on the Cooks distance cutoff. However, we have the points 39 and 83 which are high leverage points and are the closest to being influential.
Analizing the data we observe that the point 39 corresponts to the heaviest individual (having a 100 pound gap), so that translates to having high leverage. Moreover, as we saw in the stundentized vs fitted plot it had one of the highest residual (no so good of a fit), so that made high more influential than the rest.



```{r}
plot(bodyF.model, which=4)
```
This plot confirms our findings of not having a influential point based on Cooks distance, however we see that the point 39 has a much high value compared to the rest.


```{r}
# DFFITS
bodyF.model.extra <- data.frame(fitted.values= bodyF.model$fitted.values, dffits= dffits(bodyF.model))

bodyF.model.extra[abs(bodyF.model.extra[,"dffits"]) > dffits.cutoff,] 

pp <- ggplot(bodyF.model.extra, aes(x=fitted.values, y=dffits)) + 
  geom_point() +  geom_line(data=bodyF.model.extra, aes(x=fitted.values, y=dffits.cutoff), col="red", linetype = "dashed") +geom_line(data=bodyF.model.extra, aes(x=fitted.values, y=-dffits.cutoff), col="red", linetype = "dashed")
ggplotly(pp, tooltip="text")
```
When we analyze the dffits we observe that there are multiple pointsthat pass our threshold, which could imply that we should change the standard threshold to a more convenient one for our porpuse. Nevertheless, the only points that are considerably over the threshold are 39 and 83, which we already observed their influence by the Cooks distance.

Now that we know the influence in the fit of the points 39 and 83 we need to analyse the points and consider if they are trully an outlier or we can compare the models generated without them and consider mantaining them.


```{r}
bodyF.model.out1 <- lm(density ~ ., data = bodyF[-39,])
bodyF.model.out2 <- lm(density ~ ., data = bodyF[-83,])
bodyF.model.out3 <- lm(density ~ ., data = bodyF[c(-39 -83),])
print("All points")
summary(bodyF.model)["adj.r.squared"]
print("Without 39")
summary(bodyF.model.out1)["adj.r.squared"]
print("Without 83")
summary(bodyF.model.out2)["adj.r.squared"]
print("Without 39 and 83")
summary(bodyF.model.out3)["adj.r.squared"]
```
We see that there is no much diference in the adjusted R2 when we remove the influential points. Therefore, we conclude that we are going to maintain. In addition, we want to mention that their values seem like possible body proportions, so we might have some added interest in mantaining them.

# Multicolliniarity

Since most of our regressors correnspond to dimension of the body, then we already expect to have some correlation between them. Also we expect that the weight will be correlated to some regressors as well.


```{r}
# Correlation matrix 
ggpairs(data = bodyF[,-1]) # better in terminal
```

We can see that the most uncorrelated variable is the age, and then the height. However we have high correlations between the rest body dimensions as we expected.

```{r}
# Variance inflation factor (VIF)
# Hard Cutoff is 10, soft cuttof is 5
vif(bodyF.model) 
```
We see that weight, chest (merely), abdomen and hip surpass the cutoff of 10 in the VIF, which indicate us that these variables have a great dependance with the rest. We will iterativelly remove this variables and recalculate the VIFs untill there is no more variables with value greater than 10.

```{r}
bodyF.noWeight <- bodyF[,-3]
bodyF.model.red1 <- lm(density ~ ., data = bodyF.noWeight)
vif(bodyF.model.red1) 
```
We observe that we have reduced considerably the multicolliniarity. Now we are going to remove abdomen.

```{r}
bodyF.noWeight_noAbs <- bodyF[,c(-3, -7)]
bodyF.model.red1 <- lm(density ~ ., data = bodyF.noWeight_noAbs)
vif(bodyF.model.red1) 
```
We still have some multicolliniarity such as chest, hip and thight, but not as severe as before. Therefore we can procede to variable selection with this reduced data.

# Varaiable selection
## All regresors
### From reduced model

```{r}
all_possible_res <- ols_step_best_subset(bodyF.model.red1, metric = c("rsquare", "adjr", "cp", "aic", "sbc", "msep"))
View(all_possible_res)
plot(all_possible_res)

```


```{r}
# Best
best <-unique(c(which.max(all_possible_res$adjr), which.min(all_possible_res$cp), which.min(all_possible_res$aic), which.min(all_possible_res$sbc)))
View(all_possible_res[best, c("n", "predictors", "rsquare", "adjr", "cp", "aic", "sbc", "msep")])
```

```{r}
bodyF.formula.all_red_1 <- density ~ age + height + neck + chest + hip + thigh + forearm + wrist
bodyF.formula.all_red_2 <- density ~ age + chest + thigh + wrist
```

### From total model

```{r}
all_possible_res <- ols_step_best_subset(bodyF.model, metric = c("rsquare", "adjr", "cp", "aic", "sbc", "msep"))
View(all_possible_res)
plot(all_possible_res)

```


```{r}
# Best
which.max(all_possible_res$adjr) # Not worth adding a variable for that increase from 8
best <-unique(c(which.max(all_possible_res$adjr), which.min(all_possible_res$cp), which.min(all_possible_res$aic), which.min(all_possible_res$sbc)))
View(all_possible_res[best, c("n", "predictors", "rsquare", "adjr", "cp", "aic", "sbc", "msep")])
```

```{r}
bodyF.formula.all_1 <- density ~ age + weight + neck + abdomen + hip + thigh + forearm + wrist
bodyF.formula.all_2 <- density ~ weight + abdomen + forearm + wrist
```

## Forward
### From reduced model

```{r}
ford.res <- regsubsets(density~., data = bodyF.noWeight_noAbs, nvmax = 11, method="forward")
fordward.summary <- summary(ford.res)
fordward.summary
```

```{r}
par(mfrow = c(2,2))
plot(fordward.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")

plot(fordward.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
adj_r2_max = which.max(fordward.summary$adjr2)
sprintf("Best adj_r2: %d", adj_r2_max)
points(adj_r2_max, fordward.summary$adjr2[adj_r2_max], col ="red", cex = 2, pch = 20)

plot(fordward.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
cp_min = which.min(fordward.summary$cp) 
sprintf("Best Cp: %d", cp_min)
points(cp_min, fordward.summary$cp[cp_min], col = "red", cex = 2, pch = 20)

plot(fordward.summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
bic_min = which.min(fordward.summary$bic)
sprintf("Best bic: %d", bic_min)

points(bic_min, fordward.summary$bic[bic_min], col = "red", cex = 2, pch = 20)
```
We have a new model for the best BIC:
```{r}
bodyF.formula.for_red_1 <-density ~ age + height + chest + hip + thigh + wrist
```



### From total model

```{r}
ford.res <- regsubsets(density~., data = bodyF, nvmax = 13, method="forward")
fordward.summary <- summary(ford.res)
fordward.summary
```

```{r}
par(mfrow = c(2,2))
plot(fordward.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")

plot(fordward.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
adj_r2_max = which.max(fordward.summary$adjr2)
sprintf("Best adj_r2: %d", adj_r2_max)
points(adj_r2_max, fordward.summary$adjr2[adj_r2_max], col ="red", cex = 2, pch = 20)

plot(fordward.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
cp_min = which.min(fordward.summary$cp) 
sprintf("Best Cp: %d", cp_min)
points(cp_min, fordward.summary$cp[cp_min], col = "red", cex = 2, pch = 20)

plot(fordward.summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
bic_min = which.min(fordward.summary$bic)
sprintf("Best bic: %d", bic_min)

points(bic_min, fordward.summary$bic[bic_min], col = "red", cex = 2, pch = 20)
```

```{r}
bodyF.formula.for_1 <- density ~ age + weight + neck + abdomen + hip + thigh + biceps + forearm + wrist
```
We obtain a new model (which was the one that we ommited from the previous method)

## Backward

### From reduced model
```{r}
back.res <- regsubsets(density~., data = bodyF.noWeight_noAbs, nvmax = 13, method="backward")
backward.summary <- summary(back.res)
backward.summary
```

```{r}
par(mfrow = c(2,2))
plot(backward.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")

plot(backward.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
adj_r2_max = which.max(backward.summary$adjr2)
sprintf("Best adj_r2: %d", adj_r2_max)
points(adj_r2_max, backward.summary$adjr2[adj_r2_max], col ="red", cex = 2, pch = 20)

plot(backward.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
cp_min = which.min(backward.summary$cp) 
sprintf("Best Cp: %d", cp_min)
points(cp_min, backward.summary$cp[cp_min], col = "red", cex = 2, pch = 20)

plot(backward.summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
bic_min = which.min(backward.summary$bic)
sprintf("Best bic: %d", bic_min)

points(bic_min, backward.summary$bic[bic_min], col = "red", cex = 2, pch = 20)
```
We do not obtain new models.


### From total model

```{r}
back.res <- regsubsets(density~., data = bodyF, nvmax = 13, method="backward")
backward.summary <- summary(back.res)
backward.summary
```

```{r}
par(mfrow = c(2,2))
plot(backward.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")

plot(backward.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
adj_r2_max = which.max(backward.summary$adjr2)
sprintf("Best adj_r2: %d", adj_r2_max)
points(adj_r2_max, backward.summary$adjr2[adj_r2_max], col ="red", cex = 2, pch = 20)

plot(backward.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
cp_min = which.min(backward.summary$cp) 
sprintf("Best Cp: %d", cp_min)
points(cp_min, backward.summary$cp[cp_min], col = "red", cex = 2, pch = 20)

plot(backward.summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
bic_min = which.min(backward.summary$bic)
sprintf("Best bic: %d", bic_min)

points(bic_min, backward.summary$bic[bic_min], col = "red", cex = 2, pch = 20)
```
Again, we do not obtain new models. Therefore, we can conclude that we could have just used backward instead of all subsets and gain computation efficiency.


## Lasso
```{r}
x_var <- data.matrix(bodyF[, -1])
y_var <- data.matrix(bodyF[, 1])

bodyF.lasso <- cv.glmnet(x_var, y_var, alpha=1, nfolds = 4)

plot(bodyF.lasso)
bodyF.lasso$lambda.1se  # one standard deviation away from minimizing lambda
coefs.lasso <- coef(bodyF.lasso)  # coefficients for lambda.1se
coefs.lasso[rowSums(coefs.lasso) != 0,] 
```

```{r}
bodyF.formula.lasso <- density ~ age + height + neck + abdomen + wrist 
```


## Best based on Cross validation
We are going to use cross validation to calculate the predicted MSE of all models and choose the best one.

```{r}
train.control <- trainControl(method = "cv", number = 4)
```

```{r}
num <- c(0, 0, 0, 0, 0, 0, 0)
for (i in 1:100){
  cv <- train(bodyF.formula.all_1, data = bodyF, method = "lm", trControl = train.control)
  res <- cv$results
  
  cv <- train(bodyF.formula.all_2, data = bodyF, method = "lm", trControl = train.control)
  res <- rbind(res, cv$results)
  
  cv <- train(bodyF.formula.all_red_1, data = bodyF.noWeight_noAbs, method = "lm", trControl = train.control)
  res <- rbind(res, cv$results)
  
  cv <- train(bodyF.formula.all_red_2, data = bodyF.noWeight_noAbs, method = "lm", trControl = train.control)
  res <- rbind(res, cv$results)
  
  cv <- train(bodyF.formula.for_1, data = bodyF, method = "lm", trControl = train.control)
  res <- rbind(res, cv$results)
  
  cv <- train(bodyF.formula.for_red_1, data = bodyF.noWeight_noAbs, method = "lm", trControl = train.control)
  res <- rbind(res, cv$results)
  
  cv <- train(bodyF.formula.lasso, data = bodyF, method = "lm", trControl = train.control)
  res <- rbind(res, cv$results)

  
  num[which.min(res$RMSE)] <- num[which.min(res$RMSE)] + 1
  num[which.max(res$Rsquared)] <- num[which.max(res$Rsquared)] + 1
}
res
res[which.max(num), ]
```

```{r}
print("Best model formula")
bodyF.formula.all_1
```



# Bootstrap assesment of the model

```{r}
bodyF.model.best <- lm(bodyF.formula.all_1, data=bodyF)
plot(bodyF.model.best)
summary(bodyF.model.best)
```



```{r}
vif(bodyF.model.best)
```

## Ridge
```{r}
x_var <- data.matrix(bodyF[, c("age", "weight", "neck", "abdomen", "hip", "thigh", "forearm", "wrist")])
y_var <- data.matrix(bodyF[, 1])

bodyF.ridge <- cv.glmnet(x_var, y_var, alpha=0, nfolds = 4)

plot(bodyF.ridge)
bodyF.ridge$lambda.1se  # one standard deviation away from minimizing lambda
coef(bodyF.ridge)
coef(bodyF.model.best)
```


## Bootstrap coefs

```{r}
n <- 1000
bodyF.model.bootstrapresiduals <- Boot(bodyF.model.best, R=n, method="residual") 


summary(bodyF.model.bootstrapresiduals)


# hist(df00.model2.bootstrapresiduals, legend="separate")
hist(bodyF.model.bootstrapresiduals, estDensity=FALSE, estNormal = FALSE, ci="none")

Confint(bodyF.model.bootstrapresiduals, level=.95, type="perc")
```

